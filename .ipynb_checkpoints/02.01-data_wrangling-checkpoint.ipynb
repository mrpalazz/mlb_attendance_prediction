{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all columns from the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_team_data(team_name = None, year_start = None, year_end = None):\n",
    "    teams = team_name\n",
    "    print(teams)\n",
    "    years = np.arange(year_start,year_end+1)\n",
    "    sub_df = pd.DataFrame()\n",
    "\n",
    "    for team in teams:\n",
    "        print(\"Scraping data for: \", team, \"from year:\", year_start, \"to year:\", year_end)\n",
    "        for year in years:\n",
    "            try:\n",
    "                year = str(year)\n",
    "                try:\n",
    "                    url = 'https://www.baseball-reference.com/teams/' + team + '/' + year + '-schedule-scores.shtml'\n",
    "                    page = urllib.request.urlopen(url)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                soup = BeautifulSoup(page, 'html.parser')\n",
    "                table = soup.find(\"table\", {'id': 'team_schedule'})\n",
    "                table_rows = table.find_all('tr')\n",
    "                output = []\n",
    "                for tr in table_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    row = [tr.text for tr in td]\n",
    "                    output.append(row)\n",
    "                df = pd.DataFrame(output)\n",
    "                #print(df.head())\n",
    "                df = df[[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "                df.columns = ['date', 'team', 'home-away-indicator', 'opponent',  'win-loss-tie', 'runs',\n",
    "                              'runs_allowed', 'innings', 'record', 'rank', 'gb', 'win', 'loss', 'save',\n",
    "                              'time', 'day_night', 'attendance', 'cLI', 'streak']\n",
    "                df.dropna(how='all', axis=0, inplace=True)\n",
    "                df['year'] = int(year)\n",
    "                df['home_team'] = team\n",
    "                df.drop(columns= ['team'], inplace=True)\n",
    "                sub_df = pd.concat([sub_df, df], axis=0)\n",
    "            except Exception as e:\n",
    "                print(year)\n",
    "                print(team)\n",
    "                print(e)\n",
    "                pass\n",
    "    return sub_df\n",
    "\n",
    "#Adapted From:\n",
    "#https://github.com/OlivierLej/DataIsTheNewOil/blob/4b8bc4d5e56a7c5442843cf2ad1d0a0971f0945e/scraping_baseballreference.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_home_games(data = None):\n",
    "    home_games = sub_df.loc[sub_df['xhome-away-indicator' ]== '']\n",
    "    return home_games\n",
    "\n",
    "def partition_away_games(data=None):\n",
    "    away_games = sub_df.loc[sub_df['home-away-indicator' ]== '@']\n",
    "    return away_games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental data cleanup on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NYM', 'ATL', 'PHI', 'WSN', 'MIA', 'MIL', 'CIN', 'CHC', 'STL', 'PIT', 'SFG', 'LAD', 'SDP', 'COL', 'ARI', 'BOS', 'TBR', 'NYY', 'TOR', 'BAL', 'CHW', 'CLE', 'DET', 'MIN', 'KCR', 'HOU', 'OAK', 'SEA', 'LAA', 'TEX']\n",
      "Scraping data for:  NYM from year: 2015 to year: 2021\n",
      "Scraping data for:  ATL from year: 2015 to year: 2021\n",
      "Scraping data for:  PHI from year: 2015 to year: 2021\n",
      "Scraping data for:  WSN from year: 2015 to year: 2021\n",
      "Scraping data for:  MIA from year: 2015 to year: 2021\n",
      "Scraping data for:  MIL from year: 2015 to year: 2021\n",
      "Scraping data for:  CIN from year: 2015 to year: 2021\n",
      "Scraping data for:  CHC from year: 2015 to year: 2021\n",
      "Scraping data for:  STL from year: 2015 to year: 2021\n",
      "Scraping data for:  PIT from year: 2015 to year: 2021\n",
      "Scraping data for:  SFG from year: 2015 to year: 2021\n",
      "Scraping data for:  LAD from year: 2015 to year: 2021\n",
      "Scraping data for:  SDP from year: 2015 to year: 2021\n",
      "Scraping data for:  COL from year: 2015 to year: 2021\n",
      "Scraping data for:  ARI from year: 2015 to year: 2021\n",
      "Scraping data for:  BOS from year: 2015 to year: 2021\n",
      "Scraping data for:  TBR from year: 2015 to year: 2021\n",
      "Scraping data for:  NYY from year: 2015 to year: 2021\n"
     ]
    }
   ],
   "source": [
    "data = extract_team_data(team_name=['NYM', 'ATL', 'PHI',\n",
    "                                     'WSN', 'MIA', 'MIL',\n",
    "                                     'CIN', 'CHC', 'STL',\n",
    "                                     'PIT', 'SFG', 'LAD',\n",
    "                                     'SDP', 'COL', 'ARI',\n",
    "                                     'BOS', 'TBR', 'NYY',\n",
    "                                     'TOR', 'BAL', 'CHW',\n",
    "                                     'CLE', 'DET', 'MIN',\n",
    "                                     'KCR', 'HOU', 'OAK',\n",
    "                                     'SEA', 'LAA', 'TEX'], \n",
    "                                     year_start=2015,year_end=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.attendance = data.attendance.str.replace(',','')\n",
    "data.attendance = pd.to_numeric(data['attendance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split out the date information into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_thangs = data.date.str.split(\" \", expand=True)\n",
    "data['day'] = date_thangs[0].str.replace(\",\", \" \").str.strip()\n",
    "data.day.unique()\n",
    "data['month'] = date_thangs[1]\n",
    "data['num-date'] = date_thangs[2]\n",
    "data['multi-game'] = date_thangs[3]\n",
    "data.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle((\"./mlb_data.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
